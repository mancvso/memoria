%!TEX root = memoria.tex
\chapter{Capítulo 1}

El foco de este trabajo es estudiar el comportamiento a escala de los diversos protocolos de transferencia de datos que reinan en el mercado, principalmente durante su crecimiento por demanda.

%FOCO: Estudiar cómo a se comportan los protocolos de transferencia actuales en el escalado de microservicios cuando se agregan servicios nuevos y, a la vez, aumentan las peticiones.
%Mencionar la relevancia de los blogs en la manera en la que se mueve la información relativa a nuevos avances en informática.
%http://blog.wix.engineering/2015/07/14/building-a-scalable-and-resilient-architecture/

\section{Definición del Problema}

La infraestructura necesaria para el funcionamiento de internet es costosa en términos económicos, ambientales-climáticos, políticos y operacionales.

Los sistemas que se mantienen relevantes, se ven enfrentados a una gran cantidad de elementos que aumentan la complejidad de la puesta en marcha; la modularización de sus componentes de software, falta de resilencia, ineficiencias en recursos humanos y un alto costo de planificación y desarrollo son algunos de los síntomas que dan cuenta de esta complejidad que emerge al distribuir sistemas.

A lo largo de los años, con la proliferación de Internet, la necesidad de distribuir sistemas ha aumentado; diversos patrones de programación y arquitecturas de software han demostrado lo desafiante que resulta crear sistemas distribuidos confiables.

% Aún más, particionar un sistema que inicialmente fue creado como una única entidad genera nuevas necesidades, a veces con mayor costo que la reformulación total del sistema.

Latinoamérica es un continente nacido de venturas españolas y portuguesas, donde el foco de inversión de las industrias tecnológicas se dicta desde el otro extremo del mundo. Allá, el 85\% del intercambio de datos se realiza en redes móviles. Una tendencia que se refleja a todos los continentes, con un 51,3\% de todo el tráfico mundial de Internet en el 2016 provino desde y hacia dispositivos móviles [1] \colorbox{pink}{cifra que aumentó X\% en un año. AGREGAR VALOR ACTUALIZADO DEL AÑO [2]}

% \colorbox{green}{GUIAR AL LECTOR AL PASEO POR LOS COMPONENTES INFORMATICOS QUE SOSTIENEN INTERNET}

La literatura actual [77] es rotunda respecto al crecimiento acelerado que tiene la información computada en todo momento; con ramas de la informática demandando una constante ingesta de datos. Las inteligencias artificiales en general, suponen un cambio que hasta ahora se comportó de manera progresiva. Sin embargo, con la proliferación de los dispositivos móviles, nos vimos más apretados en el otro extremo del intercambio de datos; las máquinas que procesan nuestras órdenes y los profesionales que las provisionan.

Internet es la lengua común del conocimiento humano, donde cada componente se construye en los cimientos del otro. Es más bien una órbita en la que transita una forma digital de representar transacciones. Afortunadamente, esta última década ha experimentado un aumento de las información que transita en protocolos más integrales y cada vez más eficientes.

La transición reciente de una gran parte del tráfico móvil a http/2 da cuenta de los recientes avances de adaptar Internet a una mayor cantidad de dispositivos conectados realizando tareas cada vez más sofisticadas.

El volumen de datos ya no sólo crece en las interfaces humano-máquina o máquina-máquina, si no también para afinar sistemas autónomos, capaces de mejorar toda actividad humana.

Inteligencias artificiales de toda índole intercambiarán hiperparámetros cada vez más valiosos, con el fin de descentralizar el proceso de aprendizaje y generación de sistemas superiores. Esta necesidad se ve acentuada hoy por el auge de las interfaces comandadas por voz, donde la elección de protocolos binarios está sujeta incluso a la elección de códecs. Son entonces, sujetos a escrutinio para determinar si son aptos para representar segura y eficientemente la diversidad de datos que interactúan con los humanos en su lenguaje natural.

% \colorbox{green}{esperar al capítulo 3 para emitir opiniones!}

Se dispara entonces la cantidad de información que será traducida a unidades computables, las cuales serán procesadas en entornos cada vez más heterogéneos.

%\colorbox{green}{componer acá: el tema es infraestructura costosa}

Existe una oscuridad informacional en las mutaciones de estas entidades de datos la cual impacta hecatómbicamente en la veracidad de los datos y de la transparencia de una decisión computada. Han sido los ecosistemas de cada patrón, librería, sistema o técnica los que se inclinan naturalmente a una comunicación más abierta y auditable que respete la privacidad individual, el acceso universal y neutralidad política, dando fruto a una inmensa cantidad de software de altísima calidad puesto a disposición de toda la humanidad, sin costo alguno y de fácil acceso, simplificados muchas veces para potenciar la creación de nuevos sistemas y por sobre todo, clasificada para afinar la curación del código requerido por sistemas autónomos de decisión.

El consumo de APIs es un mecanismo de comunicación entre sistemas distribuidos, patrón que no posee rivales a la vista. Los esfuerzos de asegurar y acelerar las bases que componen Internet se concentran en la forma de invocar esta APIs y observar el comportamiento de su resultado, enriqueciendo la semántica de tiempo de cada dato computado.

Una metáfora que se asemeja a esta situación es un cambio de quemar petróleo a utilizarlo para elaborar el plástico con el que construimos cohetes espaciales. Así de especializada se está tornando la tecnología capaz de darle valor a las información.

%\colorbox{green}{empoderamiento, comunidad}

Y cuando esa información se torna valiosa, se despiertan intereses en varias direcciones. El uso de VPNs en consumidores finales que utilizan servicios de empresas o fundaciones, refleja la preocupación observada en una población que desconfía de posibles interferencias a raíz de conflictos políticos, mermando el acceso, privacidad, seguridad e integridad de la información.

La precisión que poseen los lenguajes de programación de sistemas y frameworks de desarrollo rápido, permite generar herramientas de trazabilidad de todo protocolo binario con especificación abierta. Esto democratiza la participación de los actores del mercado y de la comunidad del código abierto; transparenta el uso de la información. Sin embargo, nuestros mejores esfuerzos aún requieren de grandes cantidades de datos viajando por redes inestables. Esta dinámica, guiada por la colaboración, permite refinar la calidad de los protocolos de comunicación, especializar las codificaciones y crear nuevas maneras de transmitir datos entre sistemas.

Las reglas que dictan el dinamismo de estos protocolos deben ser simulables e iterables a un bajo costo marginal.

Y si consideramos recursos humanos, hay varios aspectos relevantes que quedan al descubierto; por un lado, las posibilidades de hardware empujan a los lenguajes y librerías informáticas a introducir elementos primitivos que permitan la programación paralela, con el fin de explotar los recursos disponibles en la infraestructura computacional que hospede ese cómputo. Tendencias de la industria como el cómputo en la nube, redes mesh, sensores IoT, computación en borde y el hogar conectado a sistemas autónomos, etc. Todo ésto, demandando el menor consumo energético posible con la mayor resiliencia que pueda ser concebida. Por otro lado, están los equipos de desarrollo, que históricamente han sido organismos en crecimiento constante, encargados cada vez de sistemas más difíciles de manejar, donde las garantías y correcto funcionamiento poseen impactos económicos cada vez más importantes en el funcionamiento de un negocio. Estos equipos necesitan herramientas que les permitan innovar a un ritmo creciente sin elevar el total de inversión; herramientas sencillas que permitan delegar trabajo de manera granular, con baja curva de aprendizaje. Así se asegura un crecimiento económico positivo.

[1] \url{http://www.lavanguardia.com/tecnologia/internet/20161103/411541884012/internet-movil-smartphone-tablet-ordenador-mas.html}
[2] \url{https://www.statista.com/statistics/186919/number-of-internet-users-in-latin-american-countries/}

\subsection{Contexto} % (fold)
\label{sub:contexto}


Los incrementos exógenos de la Inversión Extranjera Directa sólo pueden afectar positivamente el capital por persona transitoriamente, dados los retornos decrecientes. De este modo, la única manera de afectar el crecimiento económico en el largo plazo es a través de modificar dos factores exógenos: la tecnología y el trabajo.

\url{https://www.researchgate.net/publication/251071518_Inversion_Extranjera_Directa_y_Crecimiento_Economico_en_Latinoamerica}

Evaluar contemporáneamente un paisaje tan cambiante en esta década, como lo es la programación distribuida, es muy relevante cuando Internet es sólo la antesala de el Internet de las cosas, que se observa desarrollar nichos tecnológicos especializados (como el Internet de los Automóviles) o movimientos altamente políticos y sociales, como son las redes en malla de libre acceso (como el caso de Hyperbórea). Un paisaje donde la información es validada y su valor almacenado de manera pública en una moneda.

%\colorbox{green}{costos ambientales, energ´eticos, pol´iticos}

Contar con infraestructura computacional eficiente, flexible, resiliente, democrática, autónoma y capaz de ejecutar sistemas cognitivos, es un desafío abierto en una especie que se prepara para colonizar otros planetas.

\subsubsection{Requiermientos de arquitecturas modernas}  
  \begin{itemize}
    \item Larga vida del software
    \item Priman atributos de calidad (no-funcionales)
    \item Retrasar diseño (hasta que los problemas existan)
    \item Cambios!
    \item Modularidad para automatización (build, test, deploy, comunes en integración continua)
    \item Reflejar la estructura organizacional
  \end{itemize}

  Las arquitecturas más afectadas en esta ola de transformación, son las monolíticas, que se ven apresuradas por lograr una descomposición estable, no sólo por escalar en todas las direcciones, si no por divisar oportunidades de introducir aprendizaje de máquina.

  Dado el modelo por el cual se gobierna Internet, las aplicaciones se re-arquitectan para el cómputo asíncrono y una alta disponibilidad en un abanico diverso de dispositivos conectados. Resaltan en este apartado las arquitecturas de microservicios y el cómputo sin servidor.

% subsection contexto (end)

\subsubsection{Principales transformaciones}

El impacto de Internet es transversal a los dispositivos que lo computan, consumen y generan.

\subsection{Relevancia} % (fold)
\label{sub:relevancia}

La industria chilena, siguiendo las tendencias mundiales, busca transicionar a la computación en la nube donde es pertinente, esto es, deshacerse de la infraestructura computacional privada que poseen las empresas (con ello deshacerse también de la puesta en marcha, mantenimiento, monitoreo y administración que conllevan, apuntando principalmente a una mejora económica). Apunta a utilizar servicios de terceros para sus operaciones, primando las integraciones por sobre la creación. Todo esto es impulsado por una reducción de costos operacionales y un mejor uso de los recursos de la empresa al impulsar el trabajo focalizado en el valor agregado que se busca explotar, diversificando las fuentes de datos disponibles en las decisiones.

Esta transición demanda que los sistemas estén preparados desde sus cimientos; arquitecturas informáticas que sean capaces de acompañarles y aprovechar el crecimiento o modificaciones que pudiesen experimentar no sólo de manera técnica si no en el objeto del negocio, siendo capaces de proteger y aumentar el valor de invertir en software; que sean capaces también de generar instrospección que sea útil para cumplir las garantías del sistema, detectar anomalías en el funcionamiento, minimizar dificultades en las migraciones, adaptarse a patrones de uso por parte de los usuarios, actualizaciones parciales de datos y flujos constantes de información; todo este volúmen de información disponible requiere sistemas capaces de procesar muchos más datos que los estrictamente necesarios para llevar a cabo sus negocios (los llamados meta-datos) y se espera que sean capaces no sólo de obtener conclusiones del comportamiento, si no de predecirlo y actuar de manera preventiva.

La industria mundial se ha vuelto asidua al uso de las arquitecturas de microservicios como respuesta al problema descrito. Resulta entonces pertinente ponerlas a prueba y buscar un par de combinaciones eficientes, sólidas y simples en los ámbitos de transporte y serialización; que sean capaces de satisfacer las necesidades de la industria de hoy, en el mundo de la computación sin servidor.
% subsection relevancia (end)

\section{Objetivos}

\subsection{Objetivo General}
%\colorbox{green}{Ser consistente con el nombre del tema}
Evaluar el comportamiento de los principales protocolos de transferencia de mensajes. Observar una arquitectura de microservicios que se ve enfrentada a la necesidad de escalar sus operaciones tanto en inclusión de nuevos servicios tanto como un mayor afluente de peticiones.
El resultado principal es la elaboración de un conjunto de estrategias para arquitectos de software donde se relacione el dominio del problema, su arquitectura subyacente, aplicaciones y tecnologías óptimas para llevar a cabo la solución.

\subsection{Objetivos Específicos}
Concretamente, en el marco de la creación de mensajes que deben ser transportados, comprendidos y ejecutados por varios sistemas, el trabajo espera:

\begin{itemize}
  \item Generar una retrospectiva histórica de protocolos usados en la industria
  \item Estudiar y aplicar técnicas de serialización neutral en diversos lenguajes de programación cooperantes
  \item Establecer estrategias para la elección de transporte y serialización
  \item Crear una matriz arquitectural de herramientas neutrales a los lenguajes de programación
  \item Establecer criterios para la elección de protocolos textuales o binarios
  \item Contrastar la serialización binaria y la basada en texto plano
  \item Descubrir potenciales beneficios en la optimización de recursos humanos y aplicación en sistemas autónomos
\end{itemize}

\section{Retrospectiva Histórica}

\subsection{Patrones}

\begin{itemize}
  \item RPC %como abstracción al paso real de mensajes, incluyendo serialización y deserialización así como contrato compartido (client stub) A remote invocation mechanism alone, however, is not suficient for building distributed programs. Objects that should reside on separate nodes somehow need to get to these nodes in the first place, and, having been placed onto the desired nodes, the objects need to make initial contact with each other %
  \item CORBA (OMG 2000) %CORBA is a pragmatic approach to providing a distribution infrastructure for enterprise applications, where the main focus is on interoperability between heterogeneous platforms and programming languages. As such, distribution transparency is only a minor objective of CORBA. Similar as with Java/RMI, pretty much the only thing that is indeed distribution-transparent is the actual invocation of a remote object. In practically all other areas, distribution-related issues are completely visible in the source code. A CORBA application must necessarily be completely different from a centralized program that performs the same task.%
\end{itemize}

\subsection{Arquitecturas}

\begin{itemize}
  \item SOA \colorbox{green}{PATRÓN}
  \item Monolitos
  \item Microservicios
  \item Sistemas auto-contenidos
\end{itemize}

\colorbox{green}{acerca de microservicios}
% Seriously though - for many businesses, the biggest cost for software isn't the software anymore. It's the bandwidth, hardware, CDN costs, etc. Now that everyone has a mobile device, there's just that much more traffic. And that will only get worse as your toaster gets its own internet connectivity.

% So businesses are looking to manage those costs. Specifically, they're trying to handle the business problem of \"if this thing blows up, how can I serve millions of people getting/using my software - without paying ahead of time for the servers to serve millions of people getting/using my software?\". 

% While HTTP and REST are preferred for synchronous communication, it’s becoming increasingly popular to use asynchronous communication between microservices. Many consider the Advanced Message Queuing Protocol (AMQP) standard as the preferred protocol, in this regard. Developing microservices with an asynchronous communication model, while sometimes a little more complex, can have great advantages in terms of minimizing latency and enabling event-driven interactions with applications.

% In the market today, RabbitMQ and Apache Kafka are both commonly used message bus technologies for asynchronous communication between microservices. Also, if the message-passing is done on the same host, then the containers can communicate with each other by way of system calls, as they all share the same kernel.

% While system and application requirements continue to evolve, the methodology behind how we solve these problems is often based on older models and patterns. As mentioned before, microservices architecture has its roots in models like COM, COBRA, EJB and SOA, but there are still some rules to live by when creating microservices utilizing current technologies. While the ten best practices we’ve laid out here are not entirely comprehensive, they are core strategies for creating, migrating and managing microservices.

% On the other hand, with implicit systems (where distribution is handled in the run-time system), transparency is simply not an issue. By their nature, distribution is completely invisible to the programmer, whic h mak es them app ealingly elegan t. The big problem of these platforms is their lack of e±ciency , whic h researc hers are trying to comp ensate for by ever higher degrees of clev er automatic optimizations. Despite this, implicit platforms have not really been put to use outside of academia yet

% Waldo et al. argue that distributed and non-distributed programming cannot be uniØed, and they do so with considerable rhetoric effort.

% A Note on Distributed Computing

% The di±cult part, Waldo et al. con tinue, lies in four distinct areas where the local and the distributed case are separated by insurmoun table diÆerences. ≤ Latency. A remote metho d invocation tak es between four and Øve orders of magnitude longer than a local metho d invocation, and the curren t trends in both pro cessor speed and net work latency suggest that this will not fundamen tally change in the future. As a consequence, Waldo et al. argue, not paying atten tion to distribution from the earliest phases of dev elopmen t may lead to designs with insurmoun table performance problems. It must be decided righ t from the beginning what objects can be made remote and what objects must be clustered together" (op.cit., page 5). ≤ Memory access. Direct memory addresses are not valid outside a single address space. Waldo et al. conclude that if local and distributed computing are uniØed, this means that programmers must not use address-space-relativ e pointers. However, this restriction could only be enfor ced if the abilit y to get address-space-relativ e pointers were completely remo ved from the programming language. This, on the other hand, would require pro- grammers to learn a new style of programming, and thus give up the complete transparency between local and distributed computing. ≤ Partial failur e. In a distributed system, some comp onen ts, suc h as a net work link or an individual node, may fail while others still function normally .  This is diÆeren t from the local case, where failures at the system level are alw ays total. Programmers thus have two options: they can either ignore the
% possibilit y of partial failure, resulting in eac h partial failure being unhandled and catastrophic, or they must enhance all of their interfaces to rep ort partial failures adequately , and mak e all of their code prepared for these events. This, however, would mean that local computing becomes more like distributed computing, and not the other way round. ≤ Concurr ency. A similar argumen t can be made for concurrency (parallelism). Unlik e local objects, Waldo et al. say, distributed objects must alw ays be prepared for truly parallel invocations. In a distributed system, there is an actual indeterminacy in the order of metho d invocations, while in the local case, the programmer has complete con trol over invocation order when desired. Additionally , sync hronization becomes much more diffcult in a distributed system, because there is no single point of resource allo cation or sync hronization. Under a unified model, the burden to handle this complexit y would have to be placed on all objects, not just on those where it is actually required.

    A communications protocol is a system of digital message formats and rules for exchanging those messages in or between computing systems and in telecommunications

More informally, a protocol is an agreement between two computer systems on how they will talk to each other.



